{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library installs\n",
    "\n",
    "!pip install polars\n",
    "!pip install xgboost\n",
    "!pip install scikit-learn\n",
    "!pip install joblib\n",
    "!pip install numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204983c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d7b0c",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = [\n",
    "    \"is_internal_source_ip\",\n",
    "    \"tcp_flag\", \"udp_flag\", \"icmp_flag\", \"ftp_flag\", \"http_flag\", \"https_flag\", \"ssh_flag\",\n",
    "    \"firewall_log\", \"application_log\", \"ids_log\",\n",
    "    \"bytes_transferred_scaled\",\n",
    "    \"curl_flag\", \"windows_browser_flag\", \"mac_browser_flag\", \"nmap_script_flag\", \"sqlmap_flag\",\n",
    "    \"question_mark_count\", \"dotdot_count\", \"backslash_count\", \"admin_keyword_count\", \"passwd_keyword_count\", \"bin_bash_keyword_count\", \"root_keyword_count\", \"hydra_keyword_count\"\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = \"threat_label_encoded\"\n",
    "\n",
    "# Adjusted path for notebook location (notebooks/ folder)\n",
    "FILE_PATH = \"../data/cybersecurity_threat_detection_logs.csv\"\n",
    "\n",
    "def load_data(path: str) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Load raw logs lazily using Polars.\n",
    "    \"\"\"\n",
    "    return pl.scan_csv(\n",
    "        path,\n",
    "        infer_schema_length=10_000,\n",
    "        ignore_errors=True\n",
    "    )\n",
    "\n",
    "\n",
    "def select_and_cast(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    \"\"\"Select required columns and enforce correct data types.\"\"\"\n",
    "    return (\n",
    "        df.select([\n",
    "            pl.col(\"source_ip\").cast(pl.Utf8),\n",
    "            pl.col(\"protocol\").cast(pl.Categorical),\n",
    "            pl.col(\"threat_label\").cast(pl.Categorical),\n",
    "            pl.col(\"log_type\").cast(pl.Categorical),\n",
    "            pl.col(\"bytes_transferred\").cast(pl.Int64),\n",
    "            pl.col(\"user_agent\").cast(pl.Utf8),\n",
    "            pl.col(\"request_path\").cast(pl.Utf8),\n",
    "        ])\n",
    "    )    \n",
    "\n",
    "def split_data(df: pl.DataFrame, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Split dataset into train / validation / test 70 - 10 - 20 with stratification by threat_label.\n",
    "    \"\"\"\n",
    "    groups = df.group_by(\"threat_label\")\n",
    "    train_parts = []\n",
    "    val_parts = []\n",
    "    test_parts = []\n",
    "    for group_key, subgroup in groups:\n",
    "        subgroup = subgroup.sample(n=subgroup.height, seed=seed, shuffle=True)\n",
    "        n = subgroup.height\n",
    "        train_end = int(0.7 * n)\n",
    "        val_end = int(0.8 * n)\n",
    "        train_parts.append(subgroup.slice(0, train_end))\n",
    "        val_parts.append(subgroup.slice(train_end, val_end - train_end))\n",
    "        test_parts.append(subgroup.slice(val_end, n - val_end))\n",
    "    train = pl.concat(train_parts)\n",
    "    val = pl.concat(val_parts)\n",
    "    test = pl.concat(test_parts)\n",
    "    return train, val, test\n",
    "\n",
    "def add_ip_flags(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Add internal/external IP flags for source_ip.\"\"\"\n",
    "    return df.with_columns(\n",
    "        pl.col(\"source_ip\")\n",
    "        .str.starts_with(\"192.168.\")\n",
    "        .cast(pl.Int8)\n",
    "        .alias(\"is_internal_source_ip\")\n",
    "    )\n",
    "\n",
    "def add_protocol_type_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Abstract protocol types into behavioral flags.\"\"\"\n",
    "    protocols = [\"TCP\", \"UDP\", \"ICMP\", \"FTP\", \"HTTP\", \"HTTPS\", \"SSH\"]\n",
    "    return df.with_columns([\n",
    "        (pl.col(\"protocol\") == p).cast(pl.Int8).alias(f\"{p.lower()}_flag\")\n",
    "        for p in protocols\n",
    "    ])\n",
    "\n",
    "def add_treatment_label_encoding(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Encode threat_label into numerical labels: benign=0, suspicious=1, malicious=2.\"\"\"\n",
    "    return df.with_columns(\n",
    "        pl.when(pl.col(\"threat_label\") == \"benign\").then(0)\n",
    "        .when(pl.col(\"threat_label\") == \"suspicious\").then(1)\n",
    "        .otherwise(2)\n",
    "        .cast(pl.Int8)\n",
    "        .alias(\"threat_label_encoded\")\n",
    "    )\n",
    "\n",
    "def add_log_type_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Abstract log types into behavioral flags.\"\"\"\n",
    "    return df.with_columns([\n",
    "        (pl.col(\"log_type\") == \"firewall\").cast(pl.Int8).alias(\"firewall_log\"),\n",
    "        (pl.col(\"log_type\") == \"application\").cast(pl.Int8).alias(\"application_log\"),\n",
    "        (pl.col(\"log_type\") == \"ids\").cast(pl.Int8).alias(\"ids_log\"),\n",
    "    ])\n",
    "\n",
    "def add_bytes_transferred_scaling(df: pl.DataFrame, mean: float, std: float) -> pl.DataFrame:\n",
    "    \"\"\"Standardize bytes_transferred using z-score normalization.\"\"\"\n",
    "    return df.with_columns(\n",
    "        ((pl.col(\"bytes_transferred\") - mean) / std)\n",
    "        .alias(\"bytes_transferred_scaled\")\n",
    "    )\n",
    "\n",
    "def add_user_agent_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Abstract user agent strings into behavioral flags.\"\"\"\n",
    "    ua = pl.col(\"user_agent\").str.to_lowercase()\n",
    "    return df.with_columns([\n",
    "        ua.str.contains(\"curl\", literal=True).cast(pl.Int8).alias(\"curl_flag\"),\n",
    "        ua.str.contains(\"windows\", literal=True).cast(pl.Int8).alias(\"windows_browser_flag\"),\n",
    "        ua.str.contains(\"macintosh\", literal=True).cast(pl.Int8).alias(\"mac_browser_flag\"),\n",
    "        ua.str.contains(\"nmap\", literal=True).cast(pl.Int8).alias(\"nmap_script_flag\"),\n",
    "        ua.str.contains(\"sqlmap\", literal=True).cast(pl.Int8).alias(\"sqlmap_flag\"),\n",
    "    ])\n",
    "\n",
    "def add_path_structure_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Extract structural features from request_path.\"\"\"\n",
    "    rp = pl.col(\"request_path\").str.to_lowercase()\n",
    "    return df.with_columns([\n",
    "        rp.str.count_matches(\"?\", literal=True).alias(\"question_mark_count\"),\n",
    "        rp.str.count_matches(\"..\", literal=True).alias(\"dotdot_count\"),\n",
    "        rp.str.count_matches(\"\\\\\\\\\", literal=True).alias(\"backslash_count\"),\n",
    "        rp.str.count_matches(\"admin\", literal=True).alias(\"admin_keyword_count\"),\n",
    "        rp.str.count_matches(\"passwd\", literal=True).alias(\"passwd_keyword_count\"),\n",
    "        rp.str.count_matches(\"bin/bash\", literal=True).alias(\"bin_bash_keyword_count\"),\n",
    "        rp.str.count_matches(\"root\", literal=True).alias(\"root_keyword_count\"),\n",
    "        rp.str.count_matches(\"hydra\", literal=True).alias(\"hydra_keyword_count\"),\n",
    "    ])\n",
    "\n",
    "def apply_features(df: pl.DataFrame, mean: float, std: float) -> pl.DataFrame:\n",
    "    \"\"\"Apply all feature engineering steps.\"\"\"\n",
    "    df = add_ip_flags(df)\n",
    "    df = add_protocol_type_features(df)\n",
    "    df = add_treatment_label_encoding(df)\n",
    "    df = add_log_type_features(df)\n",
    "    df = add_bytes_transferred_scaling(df, mean, std)\n",
    "    df = add_user_agent_features(df)\n",
    "    df = add_path_structure_features(df)\n",
    "    return df.select(FEATURE_COLUMNS + [TARGET_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b815af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Data Preparation\n",
    "print(\"Loading data...\")\n",
    "DF_LAZY = load_data(FILE_PATH)\n",
    "DF_LAZY = select_and_cast(DF_LAZY)\n",
    "\n",
    "# Validate schema\n",
    "required_cols = [\n",
    "    \"source_ip\", \"protocol\", \"threat_label\",\n",
    "    \"log_type\", \"bytes_transferred\", \"user_agent\", \"request_path\"\n",
    "]\n",
    "schema_names = DF_LAZY.collect_schema().names()\n",
    "for col in required_cols:\n",
    "    assert col in schema_names, f\"Missing required column: {col}\"\n",
    "print(\"Schema validation passed!\")\n",
    "\n",
    "# Collect and split\n",
    "print(\"Collecting and splitting data...\")\n",
    "df = DF_LAZY.collect()\n",
    "train, val, test = split_data(df)\n",
    "print(f\"Train: {train.height}, Val: {val.height}, Test: {test.height}\")\n",
    "\n",
    "# Compute scaling parameters from training set\n",
    "mean = train.select(pl.col(\"bytes_transferred\").mean()).item()\n",
    "std = train.select(pl.col(\"bytes_transferred\").std()).item()\n",
    "print(f\"Bytes transferred - Mean: {mean:.2f}, Std: {std:.2f}\")\n",
    "\n",
    "# Apply features\n",
    "print(\"Applying feature engineering...\")\n",
    "train = apply_features(train, mean, std)\n",
    "val = apply_features(val, mean, std)\n",
    "test = apply_features(test, mean, std)\n",
    "\n",
    "print(\"Data preparation complete!\")\n",
    "print(f\"Features: {FEATURE_COLUMNS}\")\n",
    "print(f\"Train shape: {train.shape}, Val shape: {val.shape}, Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab11108",
   "metadata": {},
   "source": [
    "Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_regression(config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Logistic Regression using SGDClassifier with 'modified_huber' loss.\n",
    "    \"\"\"\n",
    "    return SGDClassifier(\n",
    "        loss=\"modified_huber\", \n",
    "        penalty=config.get(\"penalty\", \"l2\"),\n",
    "        alpha=0.0001, \n",
    "        max_iter=2000,\n",
    "        random_state=config.get(\"random_state\", 42),\n",
    "        class_weight=config.get(\"class_weight\", None),\n",
    "        n_jobs=13 \n",
    "    )\n",
    "\n",
    "def build_random_forest(config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Docstring for build_random_forest\n",
    "    \n",
    "    :param config: Description\n",
    "    :type config: Dict[str, Any]\n",
    "    \"\"\"\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=config.get(\"n_estimators\", 300),\n",
    "        max_depth=None,\n",
    "        class_weight=config.get(\"class_weight\", None),\n",
    "        random_state=config.get(\"random_state\", 42),\n",
    "        n_jobs=13 \n",
    "    )\n",
    "\n",
    "def build_gradient_boosting(config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    XGBoost 2.0+ GPU configuration.\n",
    "    Uses tree_method='hist' with device='cuda'.\n",
    "    \"\"\"\n",
    "    is_gpu = config.get(\"device\") == \"gpu\"\n",
    "    \n",
    "    return xgb.XGBClassifier(\n",
    "        n_estimators=config.get(\"n_estimators\", 400),\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\" if is_gpu else \"cpu\",\n",
    "        n_jobs=13,\n",
    "        random_state=config.get(\"random_state\", 42)\n",
    "    )\n",
    "\n",
    "def get_model(model_name: str, config: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Docstring for get_model\n",
    "    \n",
    "    :param model_name: Description\n",
    "    :type model_name: str\n",
    "    :param config: Description\n",
    "    :type config: Dict[str, Any]\n",
    "    \"\"\"\n",
    "    if model_name == \"logistic_regression\":\n",
    "        return build_logistic_regression(config)\n",
    "    elif model_name == \"random_forest\":\n",
    "        return build_random_forest(config)\n",
    "    elif model_name == \"gradient_boosting\":\n",
    "        return build_gradient_boosting(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18135670",
   "metadata": {},
   "source": [
    "Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted paths for notebook location\n",
    "ARTIFACT_DIR = Path(\"../artifacts\")\n",
    "MODEL_OUTPUT_DIR = Path(\"../models\")\n",
    "MODEL_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def compute_class_weights(y: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Compute class weights inversely proportional to class frequency.\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    class_weight = {\n",
    "        int(cls): float(total / (len(unique) * cnt))\n",
    "        for cls, cnt in zip(unique, counts)\n",
    "    }\n",
    "    return class_weight\n",
    "\n",
    "def train_model(model_name, model_config, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a model with the given configuration.\n",
    "    \"\"\"\n",
    "    model = get_model(model_name, model_config)\n",
    "\n",
    "    with joblib.parallel_backend('loky', n_jobs=10):\n",
    "        if model_name == \"gradient_boosting\":\n",
    "            cw_dict = model_config.get(\"class_weight\")\n",
    "            if cw_dict:\n",
    "                row_weights = np.array([cw_dict[y] for y in y_train])\n",
    "                model.fit(X_train, y_train, sample_weight=row_weights)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96974e9",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_classification(\n",
    "    model,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    model_name: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained model using security-relevant metrics.\n",
    "\n",
    "    Metrics Emphasized\n",
    "    ------------------\n",
    "    - Recall (malicious)  â† highest priority\n",
    "    - Recall (suspicious)\n",
    "    - Macro F1\n",
    "    - Confusion matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : trained model\n",
    "        Must implement predict() and optionally predict_proba()\n",
    "    X_val : np.ndarray\n",
    "        Validation or test features\n",
    "    y_val : np.ndarray\n",
    "        True labels\n",
    "    model_name : str\n",
    "        Identifier for logging and reporting\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dictionary containing detailed evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Core metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_val,\n",
    "        y_pred,\n",
    "        labels=[0, 1, 2],  # benign, suspicious, malicious\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    macro_f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "\n",
    "    malicious_recall = recall[2]\n",
    "    suspicious_recall = recall[1]\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "    # Structured report\n",
    "    metrics = {\n",
    "        \"model_name\": model_name,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"malicious_recall\": malicious_recall,\n",
    "        \"suspicious_recall\": suspicious_recall,\n",
    "        \"per_class\": {\n",
    "            \"benign\": {\n",
    "                \"precision\": precision[0],\n",
    "                \"recall\": recall[0],\n",
    "                \"f1\": f1[0],\n",
    "                \"support\": support[0],\n",
    "            },\n",
    "            \"suspicious\": {\n",
    "                \"precision\": precision[1],\n",
    "                \"recall\": recall[1],\n",
    "                \"f1\": f1[1],\n",
    "                \"support\": support[1],\n",
    "            },\n",
    "            \"malicious\": {\n",
    "                \"precision\": precision[2],\n",
    "                \"recall\": recall[2],\n",
    "                \"f1\": f1[2],\n",
    "                \"support\": support[2],\n",
    "            },\n",
    "        },\n",
    "        \"confusion_matrix\": cm,\n",
    "    }\n",
    "\n",
    "    print_evaluation(metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Reporting Utilities\n",
    "\n",
    "def print_evaluation(metrics: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print a concise, security-focused evaluation summary.\n",
    "    \"\"\"\n",
    "    print(\"\\n================ Evaluation =================\")\n",
    "    print(f\"Model: {metrics['model_name']}\")\n",
    "    print(f\"Macro F1          : {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"Weighted F1       : {metrics['weighted_f1']:.4f}\")\n",
    "    print(f\"Malicious Recall  : {metrics['malicious_recall']:.4f}\")\n",
    "    print(f\"Suspicious Recall : {metrics['suspicious_recall']:.4f}\")\n",
    "    print(\"============================================\")\n",
    "\n",
    "\n",
    "# Model Comparison & Selection\n",
    "\n",
    "def summarize_results(results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Rank models and select the best one based on security priorities.\n",
    "\n",
    "    Ranking Strategy (in order)\n",
    "    ---------------------------\n",
    "    1. Highest malicious recall\n",
    "    2. Highest suspicious recall\n",
    "    3. Highest macro F1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Output from training loop in train.py\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary : dict\n",
    "        Ranked models and selected best model\n",
    "    \"\"\"\n",
    "\n",
    "    rankings = []\n",
    "\n",
    "    for model_name, payload in results.items():\n",
    "        metrics = payload[\"metrics\"]\n",
    "        rankings.append({\n",
    "            \"model\": model_name,\n",
    "            \"malicious_recall\": metrics[\"malicious_recall\"],\n",
    "            \"suspicious_recall\": metrics[\"suspicious_recall\"],\n",
    "            \"macro_f1\": metrics[\"macro_f1\"],\n",
    "        })\n",
    "\n",
    "    # Sort by security priority\n",
    "    rankings.sort(\n",
    "        key=lambda x: (\n",
    "            x[\"malicious_recall\"],\n",
    "            x[\"suspicious_recall\"],\n",
    "            x[\"macro_f1\"],\n",
    "        ),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    best_model = rankings[0][\"model\"]\n",
    "\n",
    "    print(\"\\n=========== Model Ranking ===========\")\n",
    "    for rank, entry in enumerate(rankings, start=1):\n",
    "        print(\n",
    "            f\"{rank}. {entry['model']} | \"\n",
    "            f\"MalRec={entry['malicious_recall']:.4f} | \"\n",
    "            f\"SusRec={entry['suspicious_recall']:.4f} | \"\n",
    "            f\"MacroF1={entry['macro_f1']:.4f}\"\n",
    "        )\n",
    "    print(\"====================================\")\n",
    "\n",
    "    return {\n",
    "        \"rankings\": rankings,\n",
    "        \"best_model\": best_model,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d20619",
   "metadata": {},
   "source": [
    "## Run Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0642fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X_train = train.select(FEATURE_COLUMNS).to_numpy()\n",
    "y_train = train.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "X_val = val.select(FEATURE_COLUMNS).to_numpy()\n",
    "y_val = val.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "X_test = test.select(FEATURE_COLUMNS).to_numpy()\n",
    "y_test = test.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weight = compute_class_weights(y_train)\n",
    "print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "# Define experiments\n",
    "experiments = {\n",
    "    \"logistic_regression\": {\n",
    "        \"class_weight\": class_weight,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    },\n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": 200,\n",
    "        \"class_weight\": class_weight,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    },\n",
    "    \"gradient_boosting\": {\n",
    "        \"n_estimators\": 400,\n",
    "        \"class_weight\": class_weight,\n",
    "        \"device\": \"gpu\",  # Change to \"cpu\" if no GPU available\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "for model_name, config in experiments.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    model = train_model(model_name, config, X_train, y_train)\n",
    "    metrics = evaluate_classification(model, X_val, y_val, model_name)\n",
    "    results[model_name] = {\"model\": model, \"metrics\": metrics}\n",
    "\n",
    "# Rank and select best model\n",
    "summary = summarize_results(results)\n",
    "best_model_name = summary[\"best_model\"]\n",
    "best_model = results[best_model_name][\"model\"]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Selected best model: {best_model_name}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "final_metrics = evaluate_classification(best_model, X_test, y_test, f\"{best_model_name}_test\")\n",
    "\n",
    "# Save best model and metrics\n",
    "with open(MODEL_OUTPUT_DIR / \"best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(MODEL_OUTPUT_DIR / \"final_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_metrics, f)\n",
    "\n",
    "print(f\"\\nModel saved to {MODEL_OUTPUT_DIR / 'best_model.pkl'}\")\n",
    "print(f\"Metrics saved to {MODEL_OUTPUT_DIR / 'final_metrics.pkl'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
